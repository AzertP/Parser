{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b02eb0e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Generalised LR (GLR)\n",
    "First introduced in 1965 by Knuth [1], LR is a bottom-up parsing technique that works by constructing an automaton, then traveling through it while consuming input one by one and maintaining a stack. A more detailed description about LR can be found [here](https://rahul.gopinath.org/post/2024/07/01/lr-parsing/).\n",
    "\n",
    "Creating a grammar in LR(1), or even LR(k), can be difficult. Ideally, you want your grammar to be intuitive, easily understood, and readable. This is important because you'll make mistakes or change your mind in the future, and modifying a large grammar while making sure that it is in LR(1) is a painful process. Luckily, a more powerful parser, that can handle all context-free grammar, called Generalised LR (GLR) is available. This post outlines the implementation details of two GLR variants (RNGLR and BRNGLR), both of which are presented in Economopoulos's PhD dissertation [2].\n",
    "### Preliminary\n",
    "This post assumes that the reader is already familiar with LR parsing and context-free grammar terminology (non-terminal, derivation, nullable,...).\n",
    "\n",
    "The parser implementation here uses the fuzzingbook format for input grammar. For example a grammar of the form $$\\begin{split} S &\\rightarrow A+A\\ |\\ A-A\\\\\n",
    "A &\\rightarrow a\\ |\\ b\\end{split}$$\n",
    "Is presented as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b315dc0f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "sample_grammar = {\n",
    "\t'<S>': [['<A>', '+', '<A>'],\n",
    "\t\t\t['<A>', '-', '<A>']],\n",
    "\t'<A>': [['a'], ['b']]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5b3c69",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "#### Table Generator\n",
    "This GLR implementation uses LR(1) as the base parse table, here we introduce all the needed components for an LR(1) parse table generator.\n",
    "##### Types and helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49d2e333",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "Grammar = dict[str, list[list[str]]]\n",
    "# State in the LR automaton\n",
    "State = tuple[int, set[\"Item\"]]\n",
    "\n",
    "LOGGING = False\n",
    "\n",
    "def is_nt(k: str):\n",
    "    '''\n",
    "        Check if k is a non-terminal\n",
    "    '''\n",
    "    return (k[0], k[-1]) == ('<', '>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df6885c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "##### Item class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a50951b3",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "class Item:\n",
    "    def __init__(self, lhs: str, rhs: list[str], dot: int, look_ahead: str):\n",
    "        '''\n",
    "            An item is a production of the form A -> a·b\n",
    "            \n",
    "            Args:\n",
    "                lhs: The left-hand side of the production (A)\n",
    "                rhs: The right-hand side of the production (a b)\n",
    "                dot: The position of the dot in the production (dot = 0 means A -> ·a b)\n",
    "                look_ahead: look ahead symbol\n",
    "        '''\n",
    "        self.lhs = lhs\n",
    "        self.rhs = rhs\n",
    "        self.dot = dot\n",
    "        self.look_ahead = look_ahead\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"{self.lhs} -> {' '.join(self.rhs[:self.dot])} · {' '.join(self.rhs[self.dot:])}, {self.look_ahead}\"\n",
    "        # format: A -> a · b, look_ahead\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return self.lhs == other.lhs and self.rhs == other.rhs and self.dot == other.dot and self.look_ahead == other.look_ahead\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash((self.lhs, tuple(self.rhs), self.dot, self.look_ahead))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117c456c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "##### TableGenerator class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d39a4a88",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "class TableGenerator:\n",
    "    def __init__(self, grammar: Grammar, start: str):\n",
    "        '''\n",
    "            This class is responsible for generating the right-nulled parse table.\n",
    "        '''\n",
    "\n",
    "        self.end_of_input = \"€\"\n",
    "        self.grammar = grammar\n",
    "        self.start = self.add_new_start(start)\n",
    "\n",
    "        self.rule_list = self.reformat_grammar(grammar)\n",
    "        self.nullable = TableGenerator.get_nullable(grammar)\n",
    "        self.first = self.init_FIRST(grammar)\n",
    "        self.follow = self.init_FOLLOW(grammar, self.start)\n",
    "\n",
    "        self.symbols = self.get_symbols(grammar)\n",
    "\n",
    "        self.sppf = SPPF(grammar)\n",
    "    \n",
    "    def get_symbols(self, grammar: Grammar):\n",
    "        '''\n",
    "            Return the list of all terminals, and non-terminals\n",
    "        '''\n",
    "        symbols: set[str] = set()\n",
    "        for nt in grammar:\n",
    "            for production in grammar[nt]:\n",
    "                for symbol in production:\n",
    "                    symbols.add(symbol)\n",
    "        \n",
    "        return symbols\n",
    "    \n",
    "    def add_new_start(self, start:str):\n",
    "        '''\n",
    "            Augment new start symbol <S#>\n",
    "        '''\n",
    "        new_start = ''.join([start[:-1], \"#\", start[-1:]])\n",
    "        new_production = [[start]]\n",
    "        self.grammar[new_start] = new_production\n",
    "\n",
    "        return new_start\n",
    "\n",
    "    def reformat_grammar(self, grammar: Grammar) -> list[tuple[str, list[str]]]:\n",
    "        '''\n",
    "            Transform grammar from a dictionary to a list of rules\n",
    "\n",
    "            args\n",
    "                grammar: the given grammar, in dictionary form\n",
    "            \n",
    "            return\n",
    "                A list of rules, a rule is a tuple with lhs and rhs \n",
    "        '''\n",
    "\n",
    "        rule_list = []\n",
    "\n",
    "        for nt in grammar.keys():\n",
    "            for production in grammar[nt]:\n",
    "                rule_list.append((nt, production))\n",
    "        return rule_list\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_nullable(grammar: Grammar) -> set[str]:\n",
    "        '''\n",
    "            Get nullable set, a non-terminal is called nullable if X can derive epsilon \n",
    "\n",
    "            return\n",
    "                The set of nullable non-terminals\n",
    "        '''\n",
    "        res: set[str] = set()\n",
    "        prev_size = -1\n",
    "        while (prev_size != len(res)):\n",
    "            prev_size = len(res)\n",
    "            for nt in grammar.keys():\n",
    "                for production in grammar[nt]:\n",
    "                    if (len(production) == 0):\n",
    "                        res.add(nt)\n",
    "                        continue\n",
    "                    \n",
    "                    if (all((is_nt(symbol) and symbol in res) for symbol in production)):\n",
    "                        res.add(nt)\n",
    "        \n",
    "        return res\n",
    "    \n",
    "    def init_FIRST(self, grammar: Grammar) -> dict[str, set[str]]:\n",
    "        '''\n",
    "            Calculate the FIRST set for all symbols\n",
    "            FIRST(X) is the set of terminals that can start a string derived from X\n",
    "\n",
    "            return\n",
    "                A dictionary with all the non-terminal as keys and FIRST sets as values\n",
    "        '''\n",
    "        first: dict[str, set[str]] = {}\n",
    "        for nt in grammar:\n",
    "            first[nt] = set()\n",
    "            if nt in self.nullable:\n",
    "                first[nt].add(\"epsilon\")\n",
    "        \n",
    "        changed = True\n",
    "        while (changed):\n",
    "            changed = False\n",
    "            for lhs in grammar:\n",
    "                for rhs in grammar[lhs]:\n",
    "                    # Epsilon rule, already handled above\n",
    "                    if (len(rhs) == 0):\n",
    "                        continue\n",
    "\n",
    "                    for symbol in rhs:\n",
    "                        if (is_nt(symbol)):\n",
    "                            for char in first[symbol]:\n",
    "                                if char not in first[lhs]:\n",
    "                                    first[lhs].add(char)\n",
    "                                    changed = True\n",
    "                            \n",
    "                            if symbol not in self.nullable:\n",
    "                                break\n",
    "                        # Is terminal\n",
    "                        else:\n",
    "                            if symbol not in first[lhs]:\n",
    "                                first[lhs].add(symbol)\n",
    "                                changed = True\n",
    "                            break\n",
    "    \n",
    "        return first\n",
    "    \n",
    "    def calculate_FIRST(self, symbol_list: list[str]) -> set[str]:\n",
    "        '''\n",
    "            Calculate FIRST(X), where X is one or more non-terminals/terminals sequence\n",
    "\n",
    "            args\n",
    "                symbol_list: list of symbols\n",
    "            return\n",
    "                The FIRST set\n",
    "        '''\n",
    "\n",
    "        if (len(symbol_list) == 0):\n",
    "            return {\"epsilon\"}\n",
    "\n",
    "        res = set()\n",
    "        nullable = True\n",
    "        for symbol in symbol_list:\n",
    "            if is_nt(symbol):\n",
    "                for char in self.first[symbol]:\n",
    "                    if char != \"epsilon\":\n",
    "                        res.add(char)\n",
    "                \n",
    "                if symbol not in self.nullable:\n",
    "                    nullable = False\n",
    "                    break\n",
    "            else:\n",
    "                nullable = False\n",
    "                res.add(symbol)\n",
    "                break\n",
    "\n",
    "        if nullable:\n",
    "            res.add(\"epsilon\")\n",
    "        \n",
    "        return res\n",
    "\n",
    "    \n",
    "    def init_FOLLOW(self, grammar: Grammar, start: str) -> dict[str, set[str]]:\n",
    "        '''\n",
    "            Calculate the FOLLOW set for all symbols\n",
    "            FOLLOW(A) is the set of terminals that can appear immediate after A\n",
    "            Example: S -> A a B c then a is in FOLLOW(A)\n",
    "\n",
    "            return\n",
    "                A dictionary with all the non-terminal as keys and FOLLOW sets as values\n",
    "        '''\n",
    "        follow: dict[str, set[str]] = {}\n",
    "        for nt in grammar:\n",
    "            follow[nt] = set()\n",
    "        follow[start].add(self.end_of_input)\n",
    "\n",
    "        changed = True\n",
    "        while (changed):\n",
    "            changed = False\n",
    "            for lhs in grammar:\n",
    "                for rhs in grammar[lhs]:\n",
    "                    for idx, symbol in enumerate(rhs):\n",
    "                        if not is_nt(symbol):\n",
    "                            continue\n",
    "                        \n",
    "                        # lhs -> ...By\n",
    "                        # Adding FIRST(y) to FOLLOW(B)\n",
    "                        # print(symbol + \" \" + str(rhs[idx + 1:]))\n",
    "                        first_y = self.calculate_FIRST(rhs[idx + 1:])\n",
    "                        for char in first_y:\n",
    "                            if char == \"epsilon\":\n",
    "                                continue\n",
    "                            if char not in follow[symbol]:\n",
    "                                changed = True\n",
    "                                follow[symbol].add(char)\n",
    "                        \n",
    "                        if \"epsilon\" in first_y:\n",
    "                            # Adding all symbol from FOLLOW(lhs) to FOLLOW(B)\n",
    "                            for char in follow[lhs]:\n",
    "                                if char not in follow[symbol]:\n",
    "                                    changed = True\n",
    "                                    follow[symbol].add(char)\n",
    "                        \n",
    "        return follow\n",
    "\n",
    "    def find_closure(self, items: list[Item]) -> set[Item]:\n",
    "        '''\n",
    "            Find the closure of a list of items\n",
    "\n",
    "            args\n",
    "                items: item list\n",
    "\n",
    "            return\n",
    "                The closure set of input items\n",
    "        '''\n",
    "        res = set(items)\n",
    "        prev_len = -1\n",
    "        while (prev_len != len(res)):\n",
    "            prev_len = len(res)\n",
    "            for item in res.copy():\n",
    "                # dot at the end\n",
    "                if (item.dot == len(item.rhs)):\n",
    "                    continue\n",
    "\n",
    "                next_sym = item.rhs[item.dot]\n",
    "                if not is_nt(next_sym):\n",
    "                    continue\n",
    "                \n",
    "                # X -> α·Yβ, a\n",
    "                # Calculate FIRST(βa)\n",
    "                first_set = self.calculate_FIRST(item.rhs[item.dot + 1:] + [item.look_ahead])\n",
    "                \n",
    "                for production in self.grammar[next_sym]:\n",
    "                    for look_ahead in first_set:\n",
    "                        res.add(Item(next_sym, production, 0, look_ahead))\n",
    "        return res\n",
    "    \n",
    "    def transition(self, state: list[Item], next_sym: str) -> set[Item]:\n",
    "        '''\n",
    "            Calculate the transition from a state with <next_sym> edge\n",
    "\n",
    "            arg\n",
    "                state: current state\n",
    "                next_sym: transition edge, can be terminal or non-terminal\n",
    "            return\n",
    "                Next state, with all items in a set\n",
    "        '''        \n",
    "        items = []\n",
    "        for item in state:\n",
    "            if item.dot + 1 > len(item.rhs):\n",
    "                continue\n",
    "            if (item.rhs[item.dot] == next_sym):\n",
    "                new_item = copy(item)\n",
    "                new_item.dot = new_item.dot + 1\n",
    "                items.append(new_item)\n",
    "        \n",
    "        return self.find_closure(items)\n",
    "    \n",
    "    def generate_states(self) -> tuple[list[State], dict[tuple[int, str], int]]:\n",
    "        '''\n",
    "            This function generates all the states needed for the automata\n",
    "                - State format: a tuple (state_id, set of Items)\n",
    "\n",
    "            return\n",
    "                A tuple consists of\n",
    "                - A list of states\n",
    "                - A GOTO map: a dictionary, with (id, symbol) as keys and next_id as values\n",
    "                    Example: state_1 --A--> state_2, then GOTO[(1, \"A\")] = 2\n",
    "        '''\n",
    "        # Initial state has [<S#> -> ·<S>, $] and its closure\n",
    "        initial_state = (0, self.find_closure([Item(self.start, \n",
    "                                                    self.grammar[self.start][0], \n",
    "                                                    0, \n",
    "                                                    self.end_of_input)]))\n",
    "        states: list[State] = []\n",
    "        states.append(initial_state)\n",
    "        unprocessed_states = [initial_state]\n",
    "\n",
    "        goto_map: dict[tuple[int, str], int] = {}\n",
    "        while (len(unprocessed_states) > 0):\n",
    "            top_state_id, top_state = unprocessed_states.pop()\n",
    "            for item in top_state:\n",
    "                if (item.dot == len(item.rhs)):\n",
    "                    continue\n",
    "\n",
    "                next_sym = item.rhs[item.dot]\n",
    "                next_state = self.transition(top_state, next_sym)\n",
    "\n",
    "                # Check if state already exist\n",
    "                duplicate = False\n",
    "                dup_idx = 0\n",
    "                for idx, state in states:\n",
    "                    if (all([(item in state) for item in next_state]) and \n",
    "                        len(next_state) == len(state)):\n",
    "                        dup_idx = idx\n",
    "                        duplicate = True\n",
    "                        break\n",
    "\n",
    "                if not duplicate:\n",
    "                    new_state = (len(states), next_state)\n",
    "                    states.append(new_state)\n",
    "                    unprocessed_states.append(new_state)\n",
    "                    goto_map[(top_state_id, next_sym)] = new_state[0]\n",
    "                else:\n",
    "                    goto_map[(top_state_id, next_sym)] = dup_idx\n",
    "\n",
    "        # Print result\n",
    "        if (LOGGING):\n",
    "            for state in states:\n",
    "                print(f\"State {state[0]}\")\n",
    "                for ins in state[1]:\n",
    "                    print(ins)\n",
    "                print(\"\")\n",
    "            \n",
    "            print(\"---------------\")\n",
    "            print(\"Transition map\")\n",
    "            for key, value in goto_map.items():\n",
    "                print(f\"GOTO {key} = {value}\")\n",
    "        \n",
    "        return (states, goto_map)\n",
    "\n",
    "\tdef export_to_csv(table, path: str):\n",
    "        # table = self\n",
    "        row_entries = list(table.keys())\n",
    "        symbols = list(table[0].keys())\n",
    "        header = [\"state\"] + list(table[0].keys())\n",
    "\n",
    "        with open(path, 'w') as csv_file:\n",
    "            writer = csv.writer(csv_file, delimiter=',')\n",
    "            writer.writerow(header)\n",
    "            for state_id in row_entries:\n",
    "                row = [str(state_id)]\n",
    "                for symbol in symbols:\n",
    "                    row.append('/'.join(table[state_id][symbol]))\n",
    "                writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29ba3fa",
   "metadata": {},
   "source": [
    "## Extending LR Parser\n",
    "### Eliminating Nondeterminism\n",
    "If you are familiar with LR, you probably know about *shift/reduce conflicts* (choices between shifting and reducing) and *reduce/reduce conflicts* (choices between reducing different rules), a normal LR parser cannot handle conflicts, as it does not know which choice to make. What we can do is incorporating a bit of breadth-first search, so the parser can try all options, and that is the main idea behind GLR.\n",
    "\n",
    "For example, consider the following ambiguous grammar:\n",
    "$$\\begin{split} S &\\rightarrow a \\ B \\ c & \\hspace{1cm} (1)\\\\\n",
    "S &\\rightarrow a\\ D \\ c &\\hspace{1cm} (2) \\\\\n",
    "B &\\rightarrow b &\\hspace{1cm} (3) \\\\\n",
    "D & \\rightarrow b &\\hspace{1cm} (4)\\end{split}$$\n",
    "For this grammar, the LR(1) automaton is as below:\n",
    "![sss](images/lr1_gram.png)\n",
    "And the LR(1) parse table is:\n",
    "\n",
    "| state | a   | b   | c               | $       | S   | B   | D   |\n",
    "| ----- | --- | --- | --------------- | ------- | --- | --- | --- |\n",
    "| 0     | p2  |     |                 |         | p1  |     |     |\n",
    "| 1     |     |     |                 | acc     |     |     |     |\n",
    "| 2     |     | p4  |                 |         |     | p5  | p3  |\n",
    "| 3     |     |     | p7              |         |     |     |     |\n",
    "| 4     |     |     | r(B, 3)/r(D, 4) |         |     |     |     |\n",
    "| 5     |     |     | p6              |         |     |     |     |\n",
    "| 6     |     |     |                 | r(S, 1) |     |     |     |\n",
    "| 7     |     |     |                 | r(S, 2) |     |     |     |\n",
    "\n",
    "In this table, \"$pk$\" is shift action, it means \"go to state $k$\" and $r(X, m)$ is the reduce action meaning \"reduce symbol $X$ with rule numbered $m$.\" The symbol $\\$$ is used to denote \"end of string.\" There is a reduce/reduce conflict in state 4. Let's see what happens when we try to parse the string \"$abc$\".\n",
    "\n",
    "| Step | Input | State | Stack                     | Next operation    |\n",
    "| ---- | ----- | ----- | ------------------------- | ----------------- |\n",
    "| 0    | \"\"    | 0     | $\\$, S_0$                 | $p2$              |\n",
    "| 1    | \"a\"   | 2     | $\\$, S_0, a, S_2$         | $p4$              |\n",
    "| 2    | \"ab\"  | 4     | $\\$, S_0, a, S_1, b, S_4$ | $r(B, 3)/r(D, 4)$ |\n",
    "\n",
    "A usual LR parser now has to choose between two possible reductions ($B \\rightarrow b$ and $D \\rightarrow b$). With GLR, it can attempt to try all options, but how would it do that? The simplest solution is to duplicate the stack and treat each stack as a separate process. After performing $r(B, 3)$ the stack is $\\{\\$, S_0, a, S_1, B, S_5\\}$; similarly, we obtain $\\{\\$, S_0, a, S_1, D, S_3\\}$ when $r(D, 4)$ is applied. Now we have 2 different stacks to manage, and the parse can continue to process with both stacks. However, this approach is not ideal, the number of stacks can blow up exponentially, we need something more efficient.\n",
    "### Graph-Structured Stack (GSS)\n",
    "In the above example, notice that the first four elements are the same in both stacks, therefore we can \"share\" them in a unified data structure. This is a \"Graph-Structured Stack\", or GSS, proposed by Tomita in his book [2]. \n",
    "![ddd](images/GSS_2.png)\n",
    "\n",
    "This image illustrates how the states $S_0$ and $S_1$ are shared between the two stacks. As the name suggests, our stack is now a single graph, and each element in the stack is a node. In the original Tomita's approach, elements $a$, $D$ and $B$ are individual nodes, but here we have simplified by making them the edge labels between states.\n",
    "\n",
    "The nodes are divided into $n+1$ *levels,* with $n$ as the length of the input string. GSS construction is done level by level, and a new level is created upon a *shift* action. The GSSNode data structure is as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d83729e7",
   "metadata": {
    "id": "P2ksOh3vYqN7"
   },
   "outputs": [],
   "source": [
    "class GSSNode:\n",
    "    '''\n",
    "        Represent a node in the GSS structure, nodes are identified by id\n",
    "    '''\n",
    "    def __init__(self, level: int, id: int, label):\n",
    "        self.level = level\n",
    "        self.id = id\n",
    "        self.label = label\n",
    "        self.children: list[tuple['GSSNode', 'SPPFNode']] = []\n",
    "\n",
    "    def __repr__(self):\n",
    "        repr = f\"Node({self.label})\"\n",
    "        return repr\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.id == other.id\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.id)\n",
    "\n",
    "    def add_child(self, child: 'GSSNode', edge):\n",
    "        self.children.append((child, edge))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48cdb48",
   "metadata": {},
   "source": [
    "The GSS is a bit unusual. It does not perform the \"pop\" operation like an ordinary stack. Once a node is created, it is never removed. Instead of popping $m$ nodes out of the stack, we perform a traversal of length $m$ from the original node. For example, instead of popping 2 elements from node $S_3$, we traverse down the graph with length 2 and find that node $S_0$ is our target. We define a method to perform this operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ab1e491",
   "metadata": {
    "id": "TE1Ahk43f7Dc",
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "class GSSNode(GSSNode):\n",
    "    def find_paths_with_length(self, m: int) -> set[tuple['GSSNode',...]]:\n",
    "        '''\n",
    "            Find a set of nodes with length m from the origin node,\n",
    "            return tuples of lenght m in a set, tuples contain all the labels and the destination node\n",
    "        '''\n",
    "        \n",
    "        res: set[tuple] = set()\n",
    "        def dfs(node: GSSNode, path: list[GSSNode]):\n",
    "            if (len(path) >= m):\n",
    "                res.add(tuple(path + [node]))\n",
    "                return\n",
    "\n",
    "            for child, edge in node.children:\n",
    "                dfs(child, path + [edge])\n",
    "\n",
    "        dfs(self, [])\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175dd650",
   "metadata": {},
   "source": [
    "The `find_paths_with_length()` method doesn't have to account for cycle because GSS is a directed acyclic graph, a cycle cannot exist. Finally we can have our GSS class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e825deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GSS:\n",
    "    '''\n",
    "        A Graph Structured Stack (GSS)\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        '''\n",
    "            Initialize the graph, in RNGLR, a GSS has n levels, where n is the length of input string\n",
    "\n",
    "            Each level is a set of GSSNodes, levels are stored in a list\n",
    "        '''\n",
    "        self.level: list[set[GSSNode]] = []\n",
    "        self.count = 0\n",
    "\n",
    "    def resize(self, n: int):\n",
    "        '''\n",
    "            Resize the GSS to include n levels\n",
    "        '''\n",
    "        self.level = [set() for i in range(n)]\n",
    "\n",
    "    def create_node(self, label, level: int):\n",
    "        '''\n",
    "            Create a new node with label in a specific level\n",
    "        '''\n",
    "        new_node = GSSNode(level, self.count, label)\n",
    "        self.count += 1\n",
    "        self.level[level].add(new_node)\n",
    "        return new_node\n",
    "    \n",
    "    def find_node(self, label, level: int) -> GSSNode:\n",
    "        '''\n",
    "            Find a node with label and in a specific level\n",
    "\n",
    "            return\n",
    "                GSSNode object if found, else None is returned\n",
    "        '''\n",
    "        # Can be optimized further\n",
    "        for node in self.level[level]:\n",
    "            if (node.label == label):\n",
    "                return node\n",
    "        return None\n",
    "    \n",
    "    def __repr__(self):\n",
    "        '''\n",
    "            Print the GSS structure\n",
    "        '''\n",
    "        repr = \"GSS:\\n\"\n",
    "        for idx, level in enumerate(self.level):\n",
    "            repr += f\"Level {idx}:\\n\"\n",
    "            for node in level:\n",
    "                repr += f\"    {node}\\n\"\n",
    "                for child, edge in node.children:\n",
    "                    repr += f\"        {child} - {edge}\\n\"\n",
    "        return repr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a889c75",
   "metadata": {},
   "source": [
    "[Maybe have a parse example with GSS here]\n",
    "### Shared Packed Parse Forest (SPPF)\n",
    "For practical usage, we are more interested in a full parser rather than just a recogniser. While a recogniser's output is simply a yes/no answer, a parser has to provide a full derivation path (usually in the form of the parse tree). However, a parse tree is insufficient because we are dealing with all context-free grammars, which includes ambiguous grammars; thus, multiple derivations (or even infinite ones) are possible. Instead of a parse tree, a data structure called *Shared Packed Parse Forest* (SPPF) is used.\n",
    "\n",
    "Consider the string \"abc\" in the above example, we have 2 possible derivations, resulting in 2 parse trees: \n",
    "![parse tree](images/Parse_tree.png)\n",
    "In an SPPF, we combine them into a single graph, the final result looks like this\n",
    "![sppf](images/SPPF.png)\n",
    "\n",
    "Nodes like \"S\", \"a\", \"b\" and \"c\" are shared to reduce space. Since $S$ can be derived in two ways (either $S\\rightarrow a\\ B\\ c$ or $S\\rightarrow a\\ D\\ c$), two new black nodes are created to represent different choices. These are called **packing nodes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9a53d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PackingNode:\n",
    "    def __init__(self):\n",
    "        self.edges = []\n",
    "\n",
    "    def add_edge(self, node):\n",
    "        self.edges.append(node)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"PackingNode({self.edges})\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404079c1",
   "metadata": {},
   "source": [
    "In the RNGLR algorithm, SPPF nodes are identified by (label, start position), we will discuss more about start position later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bdd6343",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPPFNode:\n",
    "    def __init__(self, id: int, label: str, start_pos:int = -1):\n",
    "        '''\n",
    "            start_pos = -1 means the node is in epsilon-SPPF\n",
    "        '''\n",
    "        self.id = id\n",
    "        self.label = label\n",
    "        self.start_pos = start_pos\n",
    "        self.children: list['SPPFNode' | PackingNode] = []\n",
    "    \n",
    "    def add_child(self, node):\n",
    "        self.children.append(node)\n",
    "    \n",
    "    def check_sequence_exists(self, nodes: list['SPPFNode']) -> bool:\n",
    "        '''\n",
    "            Check if a sequence of nodes already exists in the current node\n",
    "        '''\n",
    "        \n",
    "        # If packing nodes exist\n",
    "        if any(isinstance(child, PackingNode) for child in self.children):\n",
    "            for child in self.children:\n",
    "                if child.edges == nodes:\n",
    "                    return True\n",
    "            return False\n",
    "        \n",
    "        # No packing nodes case\n",
    "        return self.children == nodes\n",
    "    \n",
    "    def add_children(self, nodes: list['SPPFNode']):\n",
    "        '''\n",
    "            Add a list of nodes to the current node\n",
    "        '''\n",
    "        if len(self.children) == 0:\n",
    "            for node in nodes:\n",
    "                self.add_child(node)\n",
    "            return\n",
    "        \n",
    "        # If already exists, we skip\n",
    "        if self.check_sequence_exists(nodes):\n",
    "            return\n",
    "        \n",
    "        # No packing node yet\n",
    "        if not isinstance(self.children[0], PackingNode):\n",
    "            z = PackingNode()\n",
    "            for child in self.children:\n",
    "                z.add_edge(child)\n",
    "            self.children = [z]\n",
    "        \n",
    "        t = PackingNode()\n",
    "        for node in nodes:\n",
    "            t.add_edge(node)\n",
    "        self.children.append(t)\n",
    "    \n",
    "    # Nodes are identified by (label, start_pos)\n",
    "    def __hash__(self):\n",
    "        return hash((self.label, self.start_pos))\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return (self.label == other.label and self.start_pos == other.start_pos)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"SPPF Node:({self.label}, {self.start_pos})\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ccccfa",
   "metadata": {},
   "source": [
    "For the `add_children()` method, it tries to maintain the following property for every node in the SPPF:\n",
    "- Each choice is unique, there is no overlapping choice.\n",
    "- If there is only one possible choice, no *packing node* is used.\n",
    "- If there are at least 2 choices, all choices must be wrapped in *packing nodes*.\n",
    "\n",
    "And finally we have the SPPF class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d615457e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "class SPPF:\n",
    "    def __init__(self, grammar: Grammar):\n",
    "        self.grammar = grammar\n",
    "\n",
    "        # Two dictionary node_id -> Node and node_label -> node_id\n",
    "        self.epsilon_sppf, self.I = self.build_epsilon_sppf()\n",
    "\n",
    "        self.nodes: list[SPPFNode] = []\n",
    "        self.counter = 0\n",
    "    \n",
    "    def create_node(self, label: str, start_pos: int) -> SPPFNode:\n",
    "        node = SPPFNode(self.counter, label, start_pos)\n",
    "        self.counter += 1\n",
    "        self.nodes.append(node)\n",
    "        return node\n",
    "\n",
    "    def __repr__(self):\n",
    "        repr = \"SPPF:\\n\"\n",
    "        for node in self.nodes:\n",
    "            repr += f\"    {node.label}-{node.start_pos}\\n\"\n",
    "            for child in node.children:\n",
    "                if isinstance(child, PackingNode):\n",
    "                    repr += f\"        PackingNode\\n\"\n",
    "                    for edge in child.edges:\n",
    "                        repr += f\"            {edge}\\n\"\n",
    "                else: repr += f\"        {child}\\n\"\n",
    "        return repr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1b6104",
   "metadata": {},
   "source": [
    "#### Epsilon SPPF\n",
    "An SPPF tree for a nullable string or symbol is called *epsilon-SPPF* (or $\\epsilon$-SPPF). We precompute $\\epsilon$-SPPF trees for nullable non-terminals ($A \\overset{*}\\rightarrow \\epsilon$), this step is necessary for our parser later. In addition to non-terminals, we also build an $\\epsilon$-SPPF tree for every string $\\beta$ such that $\\beta\\overset{*}\\rightarrow \\epsilon$ and there exists a rule $A \\rightarrow \\alpha \\beta$ ($\\alpha \\neq \\epsilon$) in the grammar, such string $\\beta$ is also called *right-nullable*. Finally, we define $I$ as an index function which accepts a non-terminal/string and return the corresponding $\\epsilon$-SPPF root.\n",
    "\n",
    "Let's look at an example, grammar 5.3 in the dissertation:\n",
    "$$\\begin{split} S &\\rightarrow a \\ B \\ B \\ C\\\\\n",
    "B &\\rightarrow b \\ |\\ \\epsilon \\\\\n",
    "C & \\rightarrow \\epsilon\\end{split}$$\n",
    "We have $B$ and $C$ as nullable non-terminals, and the strings $BBC$ and $BC$ satisfy the conditions for $\\beta$. Therefore we build the $\\epsilon$-SPPF for $B$, $C$, $BB$ and $BBC$:\n",
    "\n",
    "![epsilonSPPF](images/epsilonSPPF.png)\n",
    "\n",
    "In this tree, vertices are indexed from 1 to 4, hence, our $I$ function can be defined with $I(B) = 1$, $I(C) = 2$, $I(BC)=3$ and $I(BBC)=4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f33640c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPPF(SPPF):\n",
    "\tdef build_epsilon_sppf(self) -> tuple[dict[int, SPPFNode], dict[str, int]]:\n",
    "\t        '''\n",
    "\t            Build an epsilon-SPPF tree\n",
    "\t\n",
    "\t            return\n",
    "\t                A tuple that contains\n",
    "\t                - All SPPFNodes created, stored in a dict\n",
    "\t                - The I function dictionary\n",
    "\t        '''\n",
    "\t        # key: node_id, value: SPPF Node\n",
    "\t        epsilon_sppf: dict[int, SPPFNode] = {}\n",
    "\t\n",
    "\t        # Create epsilon node\n",
    "\t        eps_node = SPPFNode(0, \"epsilon\")\n",
    "\t        epsilon_sppf[0] = eps_node\n",
    "\t        counter = 1\n",
    "\t\n",
    "\t        # Find a given node with label\n",
    "\t        node_with_label: dict[str, SPPFNode] = {}\n",
    "\t\n",
    "\t        nullable = RNGLRTableGenerator.get_nullable(self.grammar)\n",
    "\t\n",
    "\t        # Step 1, add all nullable symbols\n",
    "\t        # Sorted to guarantee determinism\n",
    "\t        for nt in sorted(nullable):\n",
    "\t            node = SPPFNode(counter, nt)\n",
    "\t            epsilon_sppf[counter] = node\n",
    "\t            node_with_label[nt] = node\n",
    "\t            counter += 1\n",
    "\t        \n",
    "\t        for lhs in self.grammar:\n",
    "\t            for rhs in self.grammar[lhs]:\n",
    "\t                # Epsilon rule\n",
    "\t                if len(rhs) == 0:\n",
    "\t                    node_with_label[lhs].add_child(eps_node)\n",
    "\t                # Total nullable\n",
    "\t                elif all(x in nullable for x in rhs):\n",
    "\t                    node = PackingNode()\n",
    "\t                    for nt in rhs:\n",
    "\t                        node.add_edge(node_with_label[nt])\n",
    "\t                    node_with_label[lhs].add_child(node)\n",
    "\t                # Partial nullable\n",
    "\t                else:\n",
    "\t                    for i in range(1, len(rhs)):\n",
    "\t                        partial_rhs = rhs[i:]\n",
    "\t                        if len(partial_rhs) == 0:\n",
    "\t                            continue\n",
    "\t\n",
    "\t                        if all(x in nullable for x in partial_rhs):\n",
    "\t                            label = ''.join(partial_rhs)\n",
    "\t                            if label in node_with_label:\n",
    "\t                                continue\n",
    "\t                            node = SPPFNode(counter, label)\n",
    "\t                            for x in partial_rhs:\n",
    "\t                                node.add_child(node_with_label[x])\n",
    "\t                            node_with_label[label] = node\n",
    "\t                            epsilon_sppf[counter] = node\n",
    "\t                            counter += 1\n",
    "\t\n",
    "\t        # Construct the I indexing map label -> node_id\n",
    "\t        I: dict[str, int] = {}\n",
    "\t        for label, node in node_with_label.items():\n",
    "\t            I[label] = node.id\n",
    "\t        \n",
    "\t        return (epsilon_sppf, I)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99342dad",
   "metadata": {},
   "source": [
    "### Right-Nulled GLR (RNGLR)\n",
    "In Tomita's book, he introduced 4 different algorithms. The first one only works for grammar without $\\epsilon$-rules. Algorithm 2 and 3 were intended to handle $\\epsilon$-rules but failed to deal with hidden left-recursion in grammars. Algorithm 4 (which is the full parser) inherited the same problem from algorithm 2 and 3. RNGLR is an extension to algorithm 1 to include grammars with $\\epsilon$-rules. \n",
    "#### Right-nulled parse table\n",
    "Our algorithm uses a slightly modified parse table, which is neither LR(1) nor LALR(1). This table is built upon the usual LR table, but with the addition of new reductions for \"*right-nullable*\" rules; therefore it is called *right-nulled parse table*. A *right-nullable* rule has the form $A\\rightarrow \\alpha\\beta$, where $\\beta$ can derive to $\\epsilon$. If an reduction *item* is of the form ($A\\rightarrow \\alpha \\cdot\\beta, a$), we write $r(A, m, f)$ into the parse table, with $m=|\\alpha|$ and $f=I(\\beta)$ if $m\\neq0$ and $f=I(A)$ if $m=0$.\n",
    "\n",
    "Back to grammar 5.3 \n",
    "$$\\begin{split} S &\\rightarrow a \\ B \\ B \\ C\\\\\n",
    "B &\\rightarrow b \\ |\\ \\epsilon \\\\\n",
    "C & \\rightarrow \\epsilon\\end{split}$$\n",
    "![automata](images/grammar_53_automata.png)\n",
    "\n",
    "The regular LR(1) parse table for this grammar is \n",
    "\n",
    "| State | B   | C   | S   | a   | b             | $          |\n",
    "| ----- | --- | --- | --- | --- | ------------- | ---------- |\n",
    "| 0     |     |     | p2  | p1  |               |            |\n",
    "| 1     | p4  |     |     |     | p3/r(B, 0, 1) | r(B, 0, 1) |\n",
    "| 2     |     |     |     |     |               | acc        |\n",
    "| 3     |     |     |     |     | r(B, 1, 0)    | r(B, 1, 0) |\n",
    "| 4     | p5  |     |     |     | p6            | r(B, 0, 1) |\n",
    "| 5     |     | p7  |     |     |               | r(C, 0, 2) |\n",
    "| 6     |     |     |     |     |               | r(B, 1, 0) |\n",
    "| 7     |     |     |     |     |               | r(S, 4, 0) |\n",
    "\n",
    "At cell $T(5, \\$)$, we can see that the parser is performing the reduction $C \\rightarrow \\epsilon$, in this case $m = |\\alpha| = 0$ and $I(C) = 2$, hence we write $r(C, 0, 2)$. To form a *right-nulled* parse table, we need to add more reductions for right-nullable items. As the strings $BBC$ and $BC$ are nullable, such items in this case are $S\\rightarrow a\\cdot B\\ B\\ C$, $S\\rightarrow a\\ B\\cdot B\\ C$ and $S\\rightarrow a\\ B\\ B \\cdot C$. Three new reductions are added into the table:\n",
    "\n",
    "| State | B   | C   | S   | a   | b             | $                      |\n",
    "| ----- | --- | --- | --- | --- | ------------- | ---------------------- |\n",
    "| 0     |     |     | p2  | p1  |               |                        |\n",
    "| 1     | p4  |     |     |     | p3/r(B, 0, 1) | r(B, 0, 1) /r(S, 1, 4) |\n",
    "| 2     |     |     |     |     |               | acc                    |\n",
    "| 3     |     |     |     |     | r(B, 1, 0)    | r(B, 1, 0)             |\n",
    "| 4     | p5  |     |     |     | p6            | r(B, 0, 1) /r(S,2,3)   |\n",
    "| 5     |     | p7  |     |     |               | r(C, 0, 2)/r(S, 3, 2)  |\n",
    "| 6     |     |     |     |     |               | r(B, 1, 0)             |\n",
    "| 7     |     |     |     |     |               | r(S, 4, 0)             |\n",
    "\n",
    "We have the generate parse table method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5aa403c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "class TableGenerator(TableGenerator):\n",
    "\tdef generate_parse_table(self):\n",
    "        '''\n",
    "            This function generates an LR(1) parse table for the given grammar\n",
    "\n",
    "            return\n",
    "                The parse table in the form of a 2-dimensional dictionary.\n",
    "                Usage: T[\\<state_id\\>][\\<symbol\\>], each item is a list of possible action either \"pk\" or \"r(A, p)\"\n",
    "        '''\n",
    "        states, goto_map = self.generate_states()\n",
    "\n",
    "        row_entries = [state[0] for state in states]\n",
    "        column_entries = (list(self.symbols) + [self.end_of_input])\n",
    "        column_entries.sort()\n",
    "        \n",
    "        table: dict[int, dict[str, list[str]]] = {}\n",
    "        # Init table\n",
    "        for row in row_entries:\n",
    "            table[row] = {}\n",
    "            for col in column_entries:\n",
    "                table[row][col] = []\n",
    "\n",
    "        # Add shift and goto\n",
    "        for state, symbol in goto_map.keys():\n",
    "            table[state][symbol].append(f\"p{goto_map[(state, symbol)]}\")\n",
    "\n",
    "        # Add reduce\n",
    "        for state_id, state in states:\n",
    "            for item in state:\n",
    "                # Dot at the end\n",
    "                if (item.dot == len(item.rhs)):\n",
    "                    if item.lhs == self.start:\n",
    "                        table[state_id][self.end_of_input].append(\"acc\")\n",
    "                    else:\n",
    "                        action = f\"r{item.lhs}{item.dot}.{0}\"\n",
    "                        if (item.dot == 0):\n",
    "                            action = f\"r{item.lhs}{item.dot}.{self.sppf.I[item.lhs]}\"\n",
    "                        table[state_id][item.look_ahead].append(action)\n",
    "                # Right-nulled\n",
    "                else:\n",
    "                    right_seq = item.rhs[item.dot:]\n",
    "                    if (all([sym in self.nullable for sym in right_seq])):\n",
    "                        label = ''.join(right_seq)\n",
    "                        action = \"\"\n",
    "                        if item.lhs == self.start:\n",
    "                            action = \"acc\"\n",
    "                        else:\n",
    "                            action = f\"r{item.lhs}{item.dot}.{self.sppf.I[label]}\"\n",
    "                        table[state_id][item.look_ahead].append(action)\n",
    "\n",
    "        # print table\n",
    "        if (LOGGING):\n",
    "            print(\"\\nParsing table:\\n\")\n",
    "            frmt = \"{:>12}\" * len(column_entries)\n",
    "            print(\" \", frmt.format(*column_entries), \"\\n\")\n",
    "            ptr = 0\n",
    "            for state_id in row_entries:\n",
    "                print(f\"{{:>3}}\".format('I'+str(state_id)), end=\"\")\n",
    "                for symbol in column_entries:\n",
    "                    list_opp = []\n",
    "                    for opp in table[state_id][symbol]:\n",
    "                        word = \"\"\n",
    "                        word += opp\n",
    "                        list_opp.append(word)\n",
    "                    print(f\"{{:>12}}\".format(\"/\".join(list_opp)), end=\"\")\n",
    "                print()\n",
    "        return table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d86769",
   "metadata": {},
   "source": [
    "#### The RNGLR parser\n",
    "With GSS, SPPF and the parse table ready, we are now ready to build the RNGLR parser. The parser processes input string one by one, for each symbol a new GSS level is created, it then performs every possible reduction before shifting. Reduction and shift are performed by $\\mathrm{Reducer}$ and $\\mathrm{Shifter}$ respectively. Two special bookkeeping sets $\\mathcal{Q}$ and $\\mathcal{R}$ are used to store pending shift and reduction actions. In general:\n",
    "- $\\mathcal{Q}$ stores the shift actions in the form of $(v, k)$, which means \"from node $v$ shift into the direction of $k$\" where $k$ is a terminal and $v$ is a node in GSS. Elements in $\\mathcal{Q}$ are processed by the $\\textrm{Shifter}$.\n",
    "- $\\mathcal{R}$ stores the reduction actions. Whenever a new edge between $v$ and $w$ is created in the GSS, all applicable reductions from $v$ are processed. For a reduction $r(X, m, f)$, we add $(w, X, m, f, z)$ into $\\mathcal{R}$ where $z$ is the SPPF node that between $v$ and $w$, if $m = 0$ then $z$ is the $\\epsilon$ node.\n",
    "\n",
    "In addition to $\\mathcal{Q}$ and $\\mathcal{R}$, a set $\\mathcal{N}$ is also used to bookkeep SPPF nodes, set $\\mathcal{N}$ is reset after each parser iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e9ad7e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "class RNGLRParser:\n",
    "    '''\n",
    "        The RNGLR parser\n",
    "    '''\n",
    "    def __init__(self, start: str, grammar: Grammar, table: dict[int, dict[str, list[str]]]):\n",
    "        '''\n",
    "            Initialize RNGLR\n",
    "        '''\n",
    "        self.start = self.augment_start(start)\n",
    "        self.grammar = grammar\n",
    "        self.table = table\n",
    "\n",
    "        self.input_str = \"\"\n",
    "        self.end_of_input = \"€\"\n",
    "        self.gss = GSS()\n",
    "\n",
    "        # R and Q set, respectively\n",
    "        self.reductions: list[tuple[GSSNode, str, int]] = []\n",
    "        self.shifts: list[tuple[GSSNode, int]] = []\n",
    "\n",
    "        self.accept_states: set[int] = self.get_accept_states()\n",
    "        \n",
    "        self.sppf = SPPF(grammar)\n",
    "        self.set_N: dict[tuple[str, int], SPPFNode] = {}\n",
    "\n",
    "\tdef augment_start(self, start: str):\n",
    "        '''\n",
    "            Reformat the start symbol to <S#>\n",
    "        '''\n",
    "        new_start = ''.join([start[:-1], \"#\", start[-1:]])\n",
    "        return new_start\n",
    "\n",
    "    def get_accept_states(self) -> set[int]:\n",
    "        '''\n",
    "            Get the accept states from the parsing table\n",
    "        '''\n",
    "        ret = set()\n",
    "        for state_id in self.table:\n",
    "            if self.end_of_input in self.table[state_id]:\n",
    "                for action in self.table[state_id][self.end_of_input]:\n",
    "                    if action == \"acc\":\n",
    "                        ret.add(state_id)\n",
    "        return ret\n",
    "\n",
    "    @staticmethod\n",
    "    def get_action(action: str) -> tuple[str, ...]:\n",
    "        '''\n",
    "            Parse the action string\n",
    "            \n",
    "            args\n",
    "                action: the action string, either \"pk\" or \"r\\<A\\>k\"\n",
    "            \n",
    "            return\n",
    "                - pk -> (\"p\", k)\n",
    "                - r<A>m.f -> (\"r\", \"\\<A\\>\", m, f)\n",
    "                - m, f are integers\n",
    "        '''\n",
    "        if (action == \"acc\"):\n",
    "            return (\"acc\",)\n",
    "        action_char = action[0]\n",
    "        assert(action_char == 'p' or action_char == 'r')\n",
    "\n",
    "        if (action_char == 'p'):\n",
    "            number = int(action[1:])\n",
    "            return (action_char, number)\n",
    "\n",
    "        # Case \"r\"\n",
    "        first_idx = action.find(\"<\")\n",
    "        last_idx = action.rfind(\">\")\n",
    "        symbol = action[first_idx:last_idx + 1]\n",
    "        dot_separator = action.rfind(\".\")\n",
    "        number_1 = int(action[last_idx + 1:dot_separator])\n",
    "        number_2 = int(action[dot_separator + 1:])\n",
    "        return (action_char, symbol, number_1, number_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a1bade",
   "metadata": {},
   "source": [
    "##### Parser pseudocode\n",
    "$U_i$ is level $i$ in the GSS \n",
    "**Input:** string $a_0a_1\\dots a_{n-1}$, start state $S_S$, accept state $S_A$, table $T$ \n",
    "**Parse($S$)**\n",
    "- If $n$ is 0\n",
    "\t- If $acc \\in T(S_S, \\$)$\n",
    "\t\t- return success\n",
    "\t- return failure\n",
    "- Else\n",
    "\t- Initialisation\n",
    "\t- Look at $T(S_S, a_1)$\n",
    "\t\t- Add all applicable shift actions to $\\mathcal{Q}$\n",
    "\t\t- Add all applicable reduce actions to $\\mathcal{R}$\n",
    "\t- For $i = 0$ to $n$ do\n",
    "\t\t- If $U_i$ is not empty\n",
    "\t\t\t- $\\mathcal{N} \\gets \\emptyset$\n",
    "\t\t\t- While $\\mathcal{R} \\ne \\emptyset$\n",
    "\t\t\t\t- $\\textrm{Reducer}(i)$\n",
    "\t\t\t- $\\textrm{Shifter(i)}$\n",
    "\t- If $S_A \\in U_n$\n",
    "\t\t- set SPPF root\n",
    "\t\t- return success\n",
    "\t- return failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3492c2",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "class RNGLRParser(RNGLRParser):\n",
    "\tdef parse(self, input_str: str):\n",
    "\t        '''\n",
    "\t            The RNGLR recongizer, implemented based on pseudocode by Giorgios Robert Economopoulos\n",
    "\t        '''\n",
    "\t        sppf_root = None\n",
    "\t        result = False\n",
    "\t        try:\n",
    "\t            if (len(input_str) == 0):\n",
    "\t                if \"acc\" in self.table[0][self.end_of_input]:\n",
    "\t                    sppf_root = self.sppf.epsilon_sppf[self.sppf.I[self.start]]\n",
    "\t                    result = True\n",
    "\t            else:\n",
    "\t                # Init step\n",
    "\t                input_str = input_str + self.end_of_input\n",
    "\t                self.input_str = input_str\n",
    "\t                self.gss.resize(len(input_str))\n",
    "\t                v_0 = self.gss.create_node(0, 0)\n",
    "\t\n",
    "\t                # Check T(S, a_0)\n",
    "\t                for action in self.table[0][input_str[0]]:\n",
    "\t                    action = RNGLRParser.get_action(action)\n",
    "\t                    if (action[0] == 'p'):\n",
    "\t                        self.shifts.append((v_0, action[1]))\n",
    "\t                    if (action[0] == 'r'):\n",
    "\t                        # Reduce\n",
    "\t                        if (action[2] == 0):\n",
    "\t                            # Add (v_0, X, 0, f, epsilon)\n",
    "\t                            self.add_reduction(v_0, action[1], 0, action[2], self.sppf.epsilon_sppf[0])\n",
    "\t\n",
    "\t                # Now we parse\n",
    "\t                for i in range(len(input_str)):\n",
    "\t                    if len(self.gss.level[i]) > 0:\n",
    "\t                        self.set_N = {}\n",
    "\t                        while len(self.reductions) > 0:\n",
    "\t                            self.reducer(i)\n",
    "\t                        self.shifter(i)\n",
    "\t\n",
    "\t                        # if len(self.reductions) == 0:\n",
    "\t                        #     break\n",
    "\t\n",
    "\t                # Check accept state\n",
    "\t                for state in self.accept_states:\n",
    "\t                    acc_node = self.gss.find_node(state, len(input_str) - 1)\n",
    "\t                    if acc_node is not None:\n",
    "\t                        result = True\n",
    "\t\n",
    "\t                        # Find SPPF root\n",
    "\t                        for child in acc_node.children:\n",
    "\t                            if child[0].label == v_0.label:\n",
    "\t                                sppf_root = child[1]\n",
    "\t                        \n",
    "\t                        # print(f\"SPPF root: {sppf_root}\")\n",
    "\t                        break\n",
    "\t        \n",
    "\t        except KeyError as e:\n",
    "\t            result = False\n",
    "\t        \n",
    "\t        # print(self.gss)\n",
    "\t        return (result, sppf_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5dcb16",
   "metadata": {},
   "source": [
    "##### Reducer pseudocode\n",
    "**Reducer($i$)**\n",
    "- Get top element $(v, X, m, f, y)$ from $\\mathcal{R}$\n",
    "- Find all the paths in the GSS with length $\\max(m - 1, 0)$ starting from $v$, call the set of paths $\\mathcal{X}$\n",
    "- For $path$ in $\\mathcal{X}$\n",
    "\t- Let $u$ be the final node in $path$\n",
    "\t- if $m = 0$\n",
    "\t\t- $z\\gets$ node $f$ in the $\\epsilon$-SPPF tree\n",
    "\t- else\n",
    "\t\t- Let $c$ be the level of $u$ in GSS\n",
    "\t\t- Find SPPF node $z = (X, c)$ in $\\mathcal{N}$\n",
    "\t\t\t- If not exist then create $z$ and add to $\\mathcal{N}$\n",
    "\t- Let $k$ be the label of $u$ and $pl$ be the shift action in $T(k, a_i)$\n",
    "\t- If exists node $w$ with label $l$ in GSS level $i$\n",
    "\t\t- If edge $(w, u)$ does not exist\n",
    "\t\t\t- Create edge $(w, u)$\n",
    "\t\t\t- For $r(B, t, f) \\in T(l, a_i)$\n",
    "\t\t\t\t- If $t \\neq 0$ add $(u, B, t, f, z)$ to $\\mathcal{R}$\n",
    "\t- Else\n",
    "\t\t- Create node $w$\n",
    "\t\t- Create edge $(w, u)$\n",
    "\t\t- For action in $T(l, a_i)$\n",
    "\t\t\t- If shift action $ph$\n",
    "\t\t\t\t- Add $(w, h)$ to $\\mathcal{Q}$\n",
    "\t\t\t- If reduce action $r(B, t, f)$\n",
    "\t\t\t\t- If $t = 0$\n",
    "\t\t\t\t\t- Add $(w, B, t, f, \\epsilon)$ to $\\mathcal{R}$\n",
    "\t\t\t\t- else if $m\\neq 0$\n",
    "\t\t\t\t\t- Add $(w, B, t, f, z)$ to $\\mathcal{R}$\n",
    "\t- If $m\\neq 0$\n",
    "\t\t- $nodeSequence \\gets$ $w_{m-1},\\dots,w_1$ be the edge labels on the path\n",
    "\t\t- Append $y$ to $nodeSequence$\n",
    "\t\t- If $f\\ne 0$\n",
    "\t\t\t- Append $\\epsilon$-SPPF node numbered $f$ to $nodeSequence$\n",
    "\t\t- Call $z.\\textrm{addChildren}(nodeSequence)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0b7c53",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "class RNGLRParser(RNGLRParser):\n",
    "\t    def reducer(self, i: int):\n",
    "        '''\n",
    "            The reducer, implemented based on pseudocode by Giorgios Robert Economopoulos\n",
    "        '''\n",
    "        v, X, m, f, y = self.reductions.pop()\n",
    "        # print(f\"[Reducer level ${i}] processing: \", v, X, m)\n",
    "        # find the set of nodes that can be reached from v with lenght m-1\n",
    "        paths = v.find_paths_with_length(max(0, m - 1))\n",
    "        z: SPPFNode = None\n",
    "\n",
    "        for path in paths:\n",
    "            u = path[-1]\n",
    "            k = u.label\n",
    "\n",
    "            if m == 0:\n",
    "                z = self.sppf.epsilon_sppf[f]\n",
    "            else:\n",
    "                c = u.level\n",
    "                if (X, c) not in self.set_N:\n",
    "                    z = self.sppf.create_node(X, c)\n",
    "                    self.set_N[X, c] = z\n",
    "                else:\n",
    "                    z = self.set_N[(X, c)]\n",
    "            for action in self.table[k][X]:\n",
    "                action_obj = RNGLRParser.get_action(action)\n",
    "                if action_obj[0] == 'p':\n",
    "                    w = self.gss.find_node(action_obj[1], i)\n",
    "                    if w is not None:\n",
    "                        if u not in [x[0] for x in w.children]:\n",
    "                            w.add_child(u, z)\n",
    "                            if m != 0:\n",
    "                                for action in self.table[action_obj[1]][self.input_str[i]]:\n",
    "                                    action_obj = RNGLRParser.get_action(action)\n",
    "                                    if action_obj[0] == 'r' and action_obj[2] != 0:\n",
    "                                        # (u, B, t, f, z)\n",
    "                                        self.add_reduction(u, action_obj[1], action_obj[2], action_obj[3], z)\n",
    "                    else:\n",
    "                        w = self.gss.create_node(action_obj[1], i)\n",
    "                        w.add_child(u, z)\n",
    "                        for action in self.table[action_obj[1]][self.input_str[i]]:\n",
    "                            action_obj = RNGLRParser.get_action(action)\n",
    "                            if action_obj[0] == 'p':\n",
    "                                self.shifts.append((w, action_obj[1]))\n",
    "                            if action_obj[0] == 'r':\n",
    "                                t = action_obj[2]\n",
    "                                if t == 0:\n",
    "                                    self.add_reduction(w, action_obj[1], 0, action_obj[3], self.sppf.epsilon_sppf[0])\n",
    "                                elif (m != 0):\n",
    "                                    self.add_reduction(u, action_obj[1], t, action_obj[3], z)\n",
    "            \n",
    "            if (m != 0):\n",
    "                # Add children\n",
    "                node_seq = list(path[:-1])\n",
    "                node_seq.reverse()\n",
    "                if (m != 0):\n",
    "                    node_seq.append(y)\n",
    "                if (f != 0):\n",
    "                    node_seq.append(self.sppf.epsilon_sppf[f])\n",
    "                \n",
    "                z.add_children(node_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b01162",
   "metadata": {},
   "source": [
    "### References\n",
    "[1] D. E. Knuth, “On the translation of languages from left to right,” _Information and Control_, vol. 8, no. 6, pp. 607–639, Dec. 1965, doi: [10.1016/S0019-9958(65)90426-2](https://doi.org/10.1016/S0019-9958\\(65\\)90426-2).\n",
    "\n",
    "[2] G. R. Economopoulos, “Generalised LR parsing algorithms”. Retrieved from https://core.ac.uk/download/pdf/301667613.pdf\n",
    "\n",
    "[3] M. Tomita, _Efficient Parsing for Natural Language_. Boston, MA: Springer US, 1986. doi: [10.1007/978-1-4757-1885-0](https://doi.org/10.1007/978-1-4757-1885-0)."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "id,-all",
   "formats": "md,ipynb",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
