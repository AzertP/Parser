{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b594b47",
   "metadata": {},
   "source": [
    "First introduced in 1965 by Knuth [1], LR is a bottom-up parsing technique that works by constructing an automaton, then traveling through it while consuming input one by one and maintaining a stack. A more detailed description about LR can be found [here](https://rahul.gopinath.org/post/2024/07/01/lr-parsing/).\n",
    "\n",
    "Creating a grammar in LR(1), or even LR(k), can be difficult. Ideally, you want your grammar to be intuitive, easily understood, and readable. This is important because you'll make mistakes or change your mind in the future, and modifying a large grammar while making sure that it is in LR(1) is a painful process. Luckily, a more powerful parser, that can handle all context-free grammar, called Generalised LR (GLR) is available. This post outlines the implementation details of RNGLR and BRNGLR algorithms, both of which are presented in Economopoulos's PhD dissertation [2].\n",
    "### Generalised LR\n",
    "#### Eliminating Nondeterminism\n",
    "If you are familiar with LR, you probably know about *shift/reduce conflicts* (choices between shifting and reducing) and *reduce/reduce conflicts* (choices between reducing different rules), a normal LR parser cannot handle conflicts, as it does not know which choice to make. What we can do is incorporating a bit of breadth-first search, so the parser can try all options, and that is the main idea behind GLR.\n",
    "\n",
    "For example, consider the following ambiguous grammar:\n",
    "$$\\begin{split} S &\\rightarrow a \\ B \\ c & \\hspace{1cm} (1)\\\\\n",
    "S &\\rightarrow a\\ D \\ c &\\hspace{1cm} (2) \\\\\n",
    "B &\\rightarrow b &\\hspace{1cm} (3) \\\\\n",
    "D & \\rightarrow b &\\hspace{1cm} (4)\\end{split}$$\n",
    "For this grammar, the LR(1) automaton is as below:\n",
    "![sss](images/lr1_gram.png)\n",
    "And the LR(1) parse table is:\n",
    "\n",
    "| state | a   | b   | c               | $       | S   | B   | D   |\n",
    "| ----- | --- | --- | --------------- | ------- | --- | --- | --- |\n",
    "| 0     | p2  |     |                 |         | p1  |     |     |\n",
    "| 1     |     |     |                 | acc     |     |     |     |\n",
    "| 2     |     | p4  |                 |         |     | p5  | p3  |\n",
    "| 3     |     |     | p7              |         |     |     |     |\n",
    "| 4     |     |     | r(B, 3)/r(D, 4) |         |     |     |     |\n",
    "| 5     |     |     | p6              |         |     |     |     |\n",
    "| 6     |     |     |                 | r(S, 1) |     |     |     |\n",
    "| 7     |     |     |                 | r(S, 2) |     |     |     |\n",
    "\n",
    "In this table, \"$pk$\" is shift action, it means \"go to state $k$\" and $r(X, m)$ is the reduce action meaning \"reduce symbol $X$ with rule numbered $m$.\" The symbol $\\$$ is used to denote \"end of string.\" There is a reduce/reduce conflict in state 4. Let's see what happens when we try to parse the string \"abc\".\n",
    "\n",
    "| Step | Input | State | Stack                     | Next operation  |\n",
    "| ---- | ----- | ----- | ------------------------- | --------------- |\n",
    "| 0    | \"\"    | 0     | $\\$, S_0$                 | p2              |\n",
    "| 1    | \"a\"   | 2     | $\\$, S_0, a, S_2$         | p4              |\n",
    "| 2    | \"ab\"  | 4     | $\\$, S_0, a, S_1, b, S_4$ | r(B, 3)/r(D, 4) |\n",
    "\n",
    "A usual LR parser now has to choose between two possible reductions ($B \\rightarrow b$ and $D \\rightarrow b$). With GLR, it can attempt to try all options, but how would it do that? The simplest solution is to duplicate the stack and treat each stack as a separate process. After performing r(B, 3) action the stack is $\\{\\$, S_0, a, S_1, B, S_5\\}$; similarly, we obtain $\\{\\$, S_0, a, S_1, D, S_3\\}$ when r(D, 4) is applied. However, this approach is not ideal, the number of stacks can blow up exponentially, we need something more efficient.\n",
    "#### Graph-Structured Stack (GSS)\n",
    "In the above example, notice that the first four elements are the same in both stacks, therefore we can \"share\" them in a unified data structure. This is a \"Graph-Structured Stack\", or GSS, proposed by Tomita in his book [2]. \n",
    "![ddd](images/GSS_2.png)\n",
    "\n",
    "This image illustrates how nodes $S_0$ and $S_1$ are shared between the two stacks. As the name suggests, our stacks is now a single graph, and each element in the stack is a node. In the original Tomita's approach, elements $a$, $D$ and $B$ are individual nodes, but here we have simplified by making them the edge labels between states.\n",
    "\n",
    "The nodes are divided into $n+1$ *levels,* with $n$ as the length of the input string. GSS construction is done level by level, and a new level is created upon a *shift* action. The GSSNode data structure is as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "920e0620",
   "metadata": {
    "id": "P2ksOh3vYqN7"
   },
   "outputs": [],
   "source": [
    "class GSSNode:\n",
    "    '''\n",
    "        Represent a node in the GSS structure, nodes are indentified by id\n",
    "    '''\n",
    "    def __init__(self, level: int, id: int, label):\n",
    "        self.level = level\n",
    "        self.id = id\n",
    "        self.label = label\n",
    "        self.children: list[tuple['GSSNode', 'SPPFNode']] = []\n",
    "\n",
    "    def __repr__(self):\n",
    "        repr = f\"Node({self.label})\"\n",
    "        return repr\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.id == other.id\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.id)\n",
    "\n",
    "    def add_child(self, child: 'GSSNode', edge):\n",
    "        self.children.append((child, edge))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd8ca45",
   "metadata": {},
   "source": [
    "The GSS is a bit unusual. It does not perform the \"pop\" operation like an ordinary stack. Once a node is created, it is never removed. Instead of popping $m$ nodes out of the stack, we perform a traversal of length $m$ from the original node. For example, instead of popping 2 elements from node $S_3$, we traverse down the graph with length 2 and find that node $S_0$ is our target. We define a method to perform this operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fe9bfa8",
   "metadata": {
    "id": "TE1Ahk43f7Dc"
   },
   "outputs": [],
   "source": [
    "class GSSNode(GSSNode):\n",
    "    def find_paths_with_length(self, m: int) -> set[tuple['GSSNode',...]]:\n",
    "        '''\n",
    "            Find a set of nodes with length m from the origin node,\n",
    "            return tuples of lenght m in a set, tuples contain all the labels and the destination node\n",
    "        '''\n",
    "\n",
    "        res: set[tuple] = set()\n",
    "\n",
    "        def dfs(node: GSSNode, path: list[GSSNode]):\n",
    "            if (len(path) >= m):\n",
    "                res.add(tuple(path + [node]))\n",
    "                return\n",
    "\n",
    "            for child, edge in node.children:\n",
    "                dfs(child, path + [edge])\n",
    "\n",
    "        dfs(self, [])\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13aab6a1",
   "metadata": {},
   "source": [
    "Finally we can have our GSS class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9afca8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GSS:\n",
    "    '''\n",
    "        A Graph Structured Stack (GSS)\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        '''\n",
    "            Initialize the graph, in RNGLR, a GSS has n levels, where n is the length of input string\n",
    "\n",
    "            Each level is a set of GSSNodes, levels are stored in a list\n",
    "        '''\n",
    "        self.level: list[set[GSSNode]] = []\n",
    "        self.count = 0\n",
    "\n",
    "    def resize(self, n: int):\n",
    "        '''\n",
    "            Resize the GSS to include n levels\n",
    "        '''\n",
    "        self.level = [set() for i in range(n)]\n",
    "\n",
    "    def create_node(self, label, level: int):\n",
    "        '''\n",
    "            Create a new node with label in a specific level\n",
    "        '''\n",
    "        new_node = GSSNode(level, self.count, label)\n",
    "        self.count += 1\n",
    "        self.level[level].add(new_node)\n",
    "        return new_node\n",
    "    \n",
    "    def find_node(self, label, level: int) -> GSSNode:\n",
    "        '''\n",
    "            Find a node with label and in a specific level\n",
    "\n",
    "            return\n",
    "                GSSNode object if found, else None is returned\n",
    "        '''\n",
    "        # Can be optimized further\n",
    "        for node in self.level[level]:\n",
    "            if (node.label == label):\n",
    "                return node\n",
    "        return None\n",
    "    \n",
    "    def __repr__(self):\n",
    "        '''\n",
    "            Print the GSS structure\n",
    "        '''\n",
    "        repr = \"GSS:\\n\"\n",
    "        for idx, level in enumerate(self.level):\n",
    "            repr += f\"Level {idx}:\\n\"\n",
    "            for node in level:\n",
    "                repr += f\"    {node}\\n\"\n",
    "                for child, edge in node.children:\n",
    "                    repr += f\"        {child} - {edge}\\n\"\n",
    "        return repr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fba54e6",
   "metadata": {},
   "source": [
    "[Maybe have a parse example with GSS here]\n",
    "#### Shared Packed Parse Forest (SPPF)\n",
    "For practical usage, we are more interested in a full parser rather than just a recogniser. While a recogniser's output is simply a yes/no answer, a parser has to provide a full derivation path (usually in the form of the parse tree). However, a parse tree is insufficient because we are dealing with all context-free grammars, which includes ambiguous grammars; thus, multiple derivations (or even infinite ones) are possible. Instead of a parse tree, a data structure called *Shared Packed Parse Forest* (SPPF) is used.\n",
    "\n",
    "Consider the string \"abc\" in the above example, we have 2 possible derivations, resulting in 2 parse trees: \n",
    "![parse tree](images/Parse_tree.png)\n",
    "In an SPPF, we combine them into a single graph, the final result looks like this\n",
    "![sppf](images/SPPF.png)\n",
    "\n",
    "Nodes like \"S\", \"a\", \"b\" and \"c\" are shared to reduce space. Since $S$ can be derived in two ways (either $S\\rightarrow a\\ B\\ c$ or $S\\rightarrow a\\ D\\ c$), two new black nodes are created to represent different choices. These are called **packing nodes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9180a682",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PackingNode:\n",
    "    def __init__(self):\n",
    "        self.edges = []\n",
    "\n",
    "    def add_edge(self, node):\n",
    "        self.edges.append(node)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"PackingNode({self.edges})\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85603260",
   "metadata": {},
   "source": [
    "In the RNGLR algorithm, SPPF nodes are identified by (label, start position), we will discuss more about start position later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "778d4356",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPPFNode:\n",
    "    def __init__(self, id: int, label: str, start_pos:int = -1):\n",
    "        '''\n",
    "            start_pos = -1 means the node is in epsilon-SPPF\n",
    "        '''\n",
    "        self.id = id\n",
    "        self.label = label\n",
    "        self.start_pos = start_pos\n",
    "        self.children: list['SPPFNode' | PackingNode] = []\n",
    "    \n",
    "    def add_child(self, node):\n",
    "        self.children.append(node)\n",
    "    \n",
    "    def check_sequence_exists(self, nodes: list['SPPFNode']) -> bool:\n",
    "        '''\n",
    "            Check if a sequence of nodes already exists in the current node\n",
    "        '''\n",
    "        \n",
    "        # If exist packing nodes\n",
    "        if any(isinstance(child, PackingNode) for child in self.children):\n",
    "            for child in self.children:\n",
    "                if child.edges == nodes:\n",
    "                    return True\n",
    "            return False\n",
    "        \n",
    "        # No packing nodes case\n",
    "        return self.children == nodes\n",
    "    \n",
    "    def add_children(self, nodes: list['SPPFNode']):\n",
    "        '''\n",
    "            Add a list of nodes to the current node\n",
    "        '''\n",
    "        if len(self.children) == 0:\n",
    "            for node in nodes:\n",
    "                self.add_child(node)\n",
    "            return\n",
    "        \n",
    "        # If already exists, we skip\n",
    "        if self.check_sequence_exists(nodes):\n",
    "            return\n",
    "        \n",
    "        # No packing node yet\n",
    "        if not isinstance(self.children[0], PackingNode):\n",
    "            z = PackingNode()\n",
    "            for child in self.children:\n",
    "                z.add_edge(child)\n",
    "            self.children = [z]\n",
    "        \n",
    "        t = PackingNode()\n",
    "        for node in nodes:\n",
    "            t.add_edge(node)\n",
    "        self.children.append(t)\n",
    "    \n",
    "    # Nodes are indentified by (label, start_pos)\n",
    "    def __hash__(self):\n",
    "        return hash((self.label, self.start_pos))\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return (self.label == other.label and self.start_pos == other.start_pos)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"SPPF Node:({self.label}, {self.start_pos})\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1acbca6",
   "metadata": {},
   "source": [
    "The `add_children()` method is more involved than usual. What it tries to do is making sure that all the derivations are unique, also create and manage packing nodes when necessary. Finally we have the SPPF class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "902cb933",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "ename": "TabError",
     "evalue": "inconsistent use of tabs and spaces in indentation (824756976.py, line 17)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mdef __repr__(self):\u001b[39m\n    ^\n\u001b[31mTabError\u001b[39m\u001b[31m:\u001b[39m inconsistent use of tabs and spaces in indentation\n"
     ]
    }
   ],
   "source": [
    "class SPPF:\n",
    "    def __init__(self, grammar: Grammar):\n",
    "        self.grammar = grammar\n",
    "\n",
    "        # Two dictionary node_id -> Node and node_label -> node_id\n",
    "        self.epsilon_sppf, self.I = self.build_epsilon_sppf()\n",
    "\n",
    "        self.nodes: list[SPPFNode] = []\n",
    "        self.counter = 0\n",
    "    \n",
    "    def create_node(self, label: str, start_pos: int) -> SPPFNode:\n",
    "        node = SPPFNode(self.counter, label, start_pos)\n",
    "        self.counter += 1\n",
    "        self.nodes.append(node)\n",
    "        return node\n",
    "\n",
    "\tdef __repr__(self):\n",
    "        repr = \"SPPF:\\n\"\n",
    "        for node in self.nodes:\n",
    "            repr += f\"    {node.label}-{node.start_pos}\\n\"\n",
    "            for child in node.children:\n",
    "                if isinstance(child, PackingNode):\n",
    "                    repr += f\"        PackingNode\\n\"\n",
    "                    for edge in child.edges:\n",
    "                        repr += f\"            {edge}\\n\"\n",
    "                else: repr += f\"        {child}\\n\"\n",
    "        return repr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8665def",
   "metadata": {},
   "source": [
    "##### Epsilon SPPF\n",
    "We precompute SPPF trees for nullable non-terminals ($A \\overset{*}\\rightarrow \\epsilon$), they are also called $\\epsilon$-SPPF trees, this step is necessary for our parser later. In addition to non-terminals, we also build an $\\epsilon$-SPPF tree for every string $\\beta$ such that $\\beta\\overset{*}\\rightarrow \\epsilon$ and there exists a rule $A \\rightarrow \\alpha \\beta$ in the grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f531a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_epsilon_sppf(self) -> tuple[dict[int, SPPFNode], dict[str, int]]:\n",
    "        '''\n",
    "            Build an epsilon-SPPF tree\n",
    "\n",
    "            return\n",
    "                A tuple that contains\n",
    "                - All SPPFNodes created, stored in a dict\n",
    "                - The I function dictionary\n",
    "        '''\n",
    "        # key: node_id, value: SPPF Node\n",
    "        epsilon_sppf: dict[int, SPPFNode] = {}\n",
    "\n",
    "        # Create epsilon node\n",
    "        eps_node = SPPFNode(0, \"epsilon\")\n",
    "        epsilon_sppf[0] = eps_node\n",
    "        counter = 1\n",
    "\n",
    "        # Find a given node with label\n",
    "        node_with_label: dict[str, SPPFNode] = {}\n",
    "\n",
    "        nullable = RNGLRTableGenerator.get_nullable(self.grammar)\n",
    "\n",
    "        # Step 1, add all nullable symbols\n",
    "        # Sorted to guarantee determinism\n",
    "        for nt in sorted(nullable):\n",
    "            node = SPPFNode(counter, nt)\n",
    "            epsilon_sppf[counter] = node\n",
    "            node_with_label[nt] = node\n",
    "            counter += 1\n",
    "        \n",
    "        for lhs in self.grammar:\n",
    "            for rhs in self.grammar[lhs]:\n",
    "                # Epsilon rule\n",
    "                if len(rhs) == 0:\n",
    "                    node_with_label[lhs].add_child(eps_node)\n",
    "                # Total nullable\n",
    "                elif all(x in nullable for x in rhs):\n",
    "                    node = PackingNode()\n",
    "                    for nt in rhs:\n",
    "                        node.add_edge(node_with_label[nt])\n",
    "                    node_with_label[lhs].add_child(node)\n",
    "                # Partial nullable\n",
    "                else:\n",
    "                    for i in range(1, len(rhs)):\n",
    "                        partial_rhs = rhs[i:]\n",
    "                        if len(partial_rhs) == 0:\n",
    "                            continue\n",
    "\n",
    "                        if all(x in nullable for x in partial_rhs):\n",
    "                            label = ''.join(partial_rhs)\n",
    "                            if label in node_with_label:\n",
    "                                continue\n",
    "                            node = SPPFNode(counter, label)\n",
    "                            for x in partial_rhs:\n",
    "                                node.add_child(node_with_label[x])\n",
    "                            node_with_label[label] = node\n",
    "                            epsilon_sppf[counter] = node\n",
    "                            counter += 1\n",
    "\n",
    "        # Construct the I indexing map label -> node_id\n",
    "        I: dict[str, int] = {}\n",
    "        for label, node in node_with_label.items():\n",
    "            I[label] = node.id\n",
    "        \n",
    "        return (epsilon_sppf, I)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e981f2a",
   "metadata": {},
   "source": [
    "#### Right-Nulled GLR (RNGLR)\n",
    "In Tomita's book, he introduced 4 different algorithms. The first one only works for grammar without $\\epsilon$-rules. Algorithm 2 and 3 were intended to handle $\\epsilon$-rules but failed to deal with hidden left-recursion in grammars. Algorithm 4 (which is the full parser) inherited the same problem from algorithm 2 and 3. RNGLR is an extension to algorithm 1 to include grammars with $\\epsilon$-rules.\n",
    "\n",
    "With GSS and SPPF defined, we are now ready to build the RNGLR parser. Our algorithm uses a slightly modified parse table, which is neither LR(1) nor LALR(1). The RNGLR table is built upon the usual LR table, but with the addition of new reductions for \"*right-nullable*\" rules. A *right-nullable* rule has the form $A\\rightarrow \\alpha\\beta$, where $\\beta$ can derive to $\\epsilon$.\n",
    "\n",
    "The accommodate for the SPPF structure, we use the following format in \n",
    "\n",
    "For this specific implementation, we are using LR(1) parse table as the base, and then add right-nulled reductions later\n",
    "\n",
    "### References\n",
    "[1] D. E. Knuth, “On the translation of languages from left to right,” _Information and Control_, vol. 8, no. 6, pp. 607–639, Dec. 1965, doi: [10.1016/S0019-9958(65)90426-2](https://doi.org/10.1016/S0019-9958\\(65\\)90426-2).\n",
    "\n",
    "[2] G. R. Economopoulos, “Generalised LR parsing algorithms”. Retrieved from https://core.ac.uk/download/pdf/301667613.pdf\n",
    "\n",
    "[3] M. Tomita, _Efficient Parsing for Natural Language_. Boston, MA: Springer US, 1986. doi: [10.1007/978-1-4757-1885-0](https://doi.org/10.1007/978-1-4757-1885-0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edb6821-fbb1-46e9-8767-86bd57a6eb4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "id,-all",
   "formats": "md,ipynb",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
